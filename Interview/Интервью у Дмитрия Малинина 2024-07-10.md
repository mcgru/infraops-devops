(перепевка Карузо Рабиновичем)

devops - это философия, идеология
Нужен ли k8s? не особо (ed: если не знаешь к чему).
? SRE - что насчёт site reliability ?
(прим: SRE - это про "быстро починить", а нам нужно - "как изначально настроить")

1. Базовые сервисы инфры:
	1.1 DNS (доменные именя, namespace, FQDN)
	1.2 NTP (независимая настройка серверов времени)
	1.3 Хранилище (ed: БД для настроек? кубера?)
2. Мониторинг
	1. По времени:
		- что происходит прямо сейчас? алёртинг? "в моменте"?
		- историческая рестроспектива для исследования трендов
	2. Метрики (уровни)
		1. системный (BareMetal, виртуалка, mem, cpu, disk, io) - zabbix
		2. сервисный (запущены ли нужные сервисы? healthcheck-и, тестирование?) - VictoriaMetrics, Prometheus
		3. прикладные (тщательные метрики от сервисов - nginx, mysql, mongodb)  - VictoriaMetrics
		4. бизнес-логика (синтетические метрики) - VictoriaMetrics
		5. событийные (рестроспективные, анализ старых логов) - VictoriaMetrics
	3. Обзорность
		1. всего сразу (one-point-of-view: и метрики, и логи, все события в системе, стек-трейсы - сложна-сложна-нипанятна)
		2. мониторинг отдельных сервисов
3. Куб - панацея ли?

!!! полезняшка: коллекция health-check'ов на все случаи жизни

Кубер - среда исполнения, соглашение о слоях абстракций.
Сетевой слой (SNI: flanel, calico, ebpf)
Задачи. А между ними - Control Plane
Магия.
Хранилище - вендорское, SS(?)
Поды - сущность. Доступ из них в хранилище - через API.
Кубер - коробка с кубиками. Слоёв много.
Утилиты: kubeadm, kubectl, kubelet
helm - препроцессор для манифестов. Генератор ямлов для куба. Метаязык для куба.
Нужен ли кубер?
Есть типы приложений - statefull (Pg, отдельно как SaaS) и stateless (nginx - любимо кубером).
Плюшки кубера: 
	- scalability
	- ресурсы - упало -> перезапуск
	- отказоустойчивость
Например, у Postgresql, MySQL нет (или плохое) горизонтального масштабирования - используются внешние утилиты (patroni). Приложения не умеют в HA, им нужны балансировщики.
А вот Elastic (Search?) понимает SQL и может масштабироваться (scaling) "искаропки", "узнаёт соседей".

??? Virtual IP, VRRP. мастер, координатор, election (минимум 3)
В кубере - etcd, упор на скорость, база с конфигами серверов. Но ему нужен балансировщик (haproxy, flanel)

Ingress, Outgress (egress).
Внутри должно быть всё изолировано, namespaces. 
Ingress - для экспозиции сервисов наружу, чтобы извне могли попасть внутрь на сервис. Iptables, NAT.
Egress - наоборот, для доступа изнутри наружу.
!!! ресурс: http://kube-vip.io
!!! Lens desktop : web-ui для управления кубом.

### Отказоустойчивость
Куб - скейлинг. не 1 инстанс, а 5.
"где?": Управление - где запущены, на каждый узел прописывать лейбы.
"что?": с какими коннфигами?
!!! полезняшка: трансляция конфига docker-compose в helm: katenary .
Требовать от написателей docker-compose сразу данных по affinity! Сколько приложение будет кушать cpu, ram, disk, io

!!! полезняшка: У docker-swarm есть PG+backup = готовое решения для репликации(?) и бакапирования (?)
Но PostgreSQL (и вообще statefull) - не для кубера. нужно затачивать и лучше держать отдельно.

Бакап кубера: restik(?), http://velero.io
Кубер = отказоустойчивость за счёт скейлинга.

### Логгирование
ELK, Clickhouse, Hadoop (нахер)
Как правило, логи все в разных форматах - тексты, json, наборы полей - всё разное (timestamp, name, service, ..., тело лога).
Правильное логгирование - это 90% затрат - на нормализацию и 10% усилий - на хранение.
ELK (ElasticSearch, Logstash, Kibana)
Vector ... (http://vector.dev) -> 
	configless, работа с потоками - и rsyslog, и nginx, и прочее, 
	аггрегируем, запихиваем в конвертер (middleware)
		плагины: log2metrics, metr2log
Классический кейс: инфра (общее), инфобез (кто, куда, секреты). Собираем всё, но инфобезное - отдельно в другой поток.
Логи лучше забирать потоками. Можно и rsyslog настроить чем-то простеньким потоком.

### Мониторинг
По Раневской: это часть инфры, но не входит в инфру. Должно стоять отдельно и в случае чего - "умереть последним".
По уровням (сис, сервис, приклад, бизнес, логи), по времени (момент, ретроспек)
Дашбординг и Алёртинг.

### Тестирование
(решили отложить на потом)
http://dbdb.io - различные виды баз (для архитекторов и разработчиков)

Далее была небольшая дискуссия с Дмитрием Николаевым про ранчер (с внешней базой), как отдельный слой api, "в случае проблем - откатить виртуалку".
немного про k3s (одно-бинарниковое, упрощённый k8s) - как набор манифестов, куб без легаси.

Далее была реплика про мощь Apache Ignite (быстрое, но прожорливое), высказано опасение по поводу языка программирования, на котором написан ignite.

...

##### Комментарии от Павла

* нужно квотировать после нагрузочного обязательно. 
* Кроме того обозначить метрики и триггеры алертов именно выделить в обязательное условие приема на саппорт (в прод).  Иначе головняки будут обязательно. 
* Также про бэкапы - нужно генерить сразу требования к описанию оных. Как то: Что именно бэкапим, каким методом: icrement, diff, full. Обязательно план восстановления с тестированием и итогами тестового восстановления.
* Это (шпаргалки по InfraOps) описано, понятное дело, чаще всего в релгаментах внутренних и регуляторке типа ЦБ, и прочем. В остальном в сети разровненная инфа, котора подразумевает наличие у админа здравого смысла и опыта коть какого-то
* Практики как таковые - ХЗ. Всё зависит от того, что бэкапить. Вангую, если важная data, database, то diff предпочтительнее, но дороже в хранении. Тут всё зависит от возможности, ресурсов и здравого смысла, опять же)) . Ну и регуляторки давления тоже.

##### Комментарии от Александра

* (про сгенерённое ИИ) Нарм, только Swarm нинужон
* И я бы добавил InfraOps и выкинул бы Jenkins
* В Performance Tuning маловато...
* И что-нибудь про постмортемы еще нужно прикрутить и про алерты

#### Аффтары
Дмитрий Малинин, Дмитрий Николаев, Alex, Павел Тихонюк, Александр Чистяков, и другие участники канала "BASH DAYS|CHAT" (слушатели)
Записал: Иван Грушин
